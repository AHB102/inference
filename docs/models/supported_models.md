There are three ways you can deploy models with Inference:

1. [Deploy a model hosted on Roboflow onto your device.](/quickstart/explore_models/)
2. [Upload suported model weights to Roboflow, then deploy to your device.](/models/from_local_weights/)
3. [Deploy a foundation model out-of-the-box.](/foundation/about/)

The pages linked above will walk you through each of these options.

## Supported Custom Models

The following custom models are supported:

- YOLOv5 (Segmentation, Object Detection)
- YOLOv7 (Segmentation)
- YOLOv8 (Classification, Segmentation, Object Detection)
- YOLO-NAS (Coming soon)

## Supported Foundation Models

- CLIP
- CogVLM
- DocTR
- Grounding DINO
- L2CS-Net
- Segment Anything
- YOLOWorld
